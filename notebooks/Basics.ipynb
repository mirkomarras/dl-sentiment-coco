{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Adaptation with Word Embeddings for Sentiment Analysis on Online Course Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import RepeatVector, dot, LSTM, Permute, merge, TimeDistributed, Bidirectional, Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, GaussianNoise\n",
    "from keras.layers import Activation, MaxoutDense, Embedding, SimpleRNN, MaxoutDense\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.engine.topology import Layer\n",
    "from keras.constraints import maxnorm\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from sklearn.manifold import TSNE\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from sklearn import preprocessing\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import sys\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Plot Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = 'embeddings/specific/word2vec/word2vec_300_50.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(embedding_path) as embedding_file:\n",
    "    for line in embedding_file:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sample embedding for \"fantastic\":', embeddings_index['fantastic'][:6], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_values, embeddings_words = np.array(list(embeddings_index.values())), np.array(list(embeddings_index.keys()))\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(embeddings_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16.5})\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "term_1 = 'fantastic'\n",
    "term_2 = 'great'\n",
    "term_3 = 'fantastic'\n",
    "\n",
    "index_term_1, = np.where(embeddings_words == term_1)\n",
    "index_term_2, = np.where(embeddings_words == term_2)\n",
    "index_term_3, = np.where(embeddings_words == term_3)\n",
    "\n",
    "plt.title(r'Sample Word Embeddings')\n",
    "plt.scatter(tsne_results[index_term_1,0], tsne_results[index_term_1,1], s=np.pi*10, linewidth=0.5, edgecolor='black', alpha=0.25, label=term_1)\n",
    "plt.scatter(tsne_results[index_term_2,0], tsne_results[index_term_2,1], s=np.pi*10, linewidth=0.5, edgecolor='black', alpha=0.25, label=term_2)\n",
    "plt.scatter(tsne_results[index_term_3,0], tsne_results[index_term_3,1], s=np.pi*10, linewidth=0.5, edgecolor='black', alpha=0.25, label=term_3)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.legend(framealpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Models and Predict Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None, bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],), initializer=self.init, name='{}_W'.format(self.name), regularizer=self.W_regularizer, constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],), initializer='zero', name='{}_b'.format(self.name), regularizer=self.b_regularizer, constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/class2/word2vec_300_30_fold0_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_path, {'Attention': Attention, 'dot_product': dot_product})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review = 'Bad course'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 500\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "all_words = [word for word, i in word_index.items()]\n",
    "max_words = len(word_index) + 1\n",
    "print('Found %s unique words.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=max_len)\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "print('* PRE-PROCESSING DATA')\n",
    "skf = StratifiedKFold(n_splits=args.n_fold)\n",
    "\n",
    "print('Scaling score values')\n",
    "if args.n_classes == 10:\n",
    "    discrete_labels = np.array([getClass10(x) for x in labels])\n",
    "elif args.n_classes == 5:\n",
    "    discrete_labels = np.array([getClass5(x) for x in labels])\n",
    "else:\n",
    "    discrete_labels = np.array([getClass2(x) for x in labels])\n",
    "\n",
    "print('Shuffling data')\n",
    "s = np.arange(data.shape[0])\n",
    "np.random.shuffle(s)\n",
    "data = data[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([test_review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The sentiment score for \"', test_review, '\" is', y_pred[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
